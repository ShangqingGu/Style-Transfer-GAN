{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "sys.path.insert(1, './code/')\n",
    "from models import Create_nets\n",
    "from optimizer import Get_loss_func, Get_optimizers\n",
    "from utils import ReplayBuffer, LambdaLR, sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Options for this script And Create Experiement Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self, **opts):\n",
    "        self.__dict__.update(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pool_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {'exp_name': 'assets/Exp1',\n",
    "        'epoch_start': 0,\n",
    "        'epoch_num': 20,\n",
    "        'data_root': './datasets/',\n",
    "        'dataset_name': 'photo2vangogh',\n",
    "        'batch_size': 1,\n",
    "        'lr': 0.0002,\n",
    "        'b1': 0.5,\n",
    "        'b2': 0.999,\n",
    "        'decay_epoch': 8,\n",
    "        'n_cpu': 4,\n",
    "        'img_height': 256,\n",
    "        'img_width': 256,\n",
    "        'input_nc_A': 3,\n",
    "        'input_nc_B': 3,\n",
    "        'sample_interval': 200,\n",
    "        'checkpoint_interval': 1,\n",
    "        'n_residual_blocks': 9,\n",
    "        'n_D_layers': 4,\n",
    "        'lambda_cyc': 10,\n",
    "        'lambda_id': 0.5,\n",
    "        'pool_size': 1,\n",
    "        'img_result_dir': 'results',\n",
    "        'model_result_dir': 'saved_models'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Options(**opts)\n",
    "os.makedirs('%s-%s/%s' % (args.exp_name, args.dataset_name, args.img_result_dir), exist_ok=True)\n",
    "os.makedirs('%s-%s/%s' % (args.exp_name, args.dataset_name, args.model_result_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, args, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.args = args\n",
    "        self.unaligned = unaligned\n",
    "        self.files_X = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
    "        self.files_Y = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_X = Image.open(self.files_X[index % len(self.files_X)])\n",
    "        if self.unaligned:\n",
    "            img_Y = Image.open(self.files_Y[random.randint(0, len(self.files_Y)-1)])\n",
    "        else:\n",
    "            img_Y = Image.open(self.files_Y[index % len(self.files_Y)] )\n",
    "\n",
    "        img_X = self.transform(img_X)\n",
    "        img_Y = self.transform(img_Y)\n",
    "\n",
    "        if self.args.input_nc_A == 1:\n",
    "            img_X = img_X.convert('L')\n",
    "\n",
    "        if self.args.input_nc_B == 1:\n",
    "            img_Y = img_Y.convert('L')\n",
    "\n",
    "        return {'X': img_X, 'Y': img_Y}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_X), len(self.files_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = [ transforms.Resize(int(args.img_height*1.12), Image.BICUBIC),\n",
    "                transforms.RandomCrop((args.img_height, args.img_width)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "\n",
    "train_dataloader = DataLoader(ImageDataset(args, \"%s/%s\" % (args.data_root, args.dataset_name), transforms_=transforms_,unaligned=True,mode='train'),\n",
    "                    batch_size=args.batch_size, shuffle=True, num_workers=args.n_cpu//2, drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(ImageDataset(args, \"%s/%s\" % (args.data_root, args.dataset_name), transforms_=transforms_, unaligned=True,mode='test'),\n",
    "                        batch_size=4, shuffle=True, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = (1, args.img_height//(2**args.n_D_layers) - 2 , args.img_width//(2**args.n_D_layers) - 2)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G__AB, D__B, G__BA, D__A = Create_nets(args)\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN, criterion_cycle, criterion_identity = Get_loss_func(args)\n",
    "# Optimizers\n",
    "optimizer_G, optimizer_D_B, optimizer_D_A = Get_optimizers(args, G__AB, G__BA, D__B, D__A )\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(args.epoch_num, args.epoch_start, args.decay_epoch).step)\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_Y_A_buffer = ReplayBuffer()\n",
    "fake_X_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] [Batch 1690/2832] [D loss: 0.352787] [G loss: 3.055143, adv: 0.512677, cycle: 0.240764, identity: 0.269649] ETA: 2:11:14.2580340"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "for epoch in range(args.epoch_start, args.epoch_num):\n",
    "    epoch_statistic = ''\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_X_A = Variable(batch['X'].type(torch.FloatTensor).cuda())\n",
    "        real_Y_B = Variable(batch['Y'].type(torch.FloatTensor).cuda())\n",
    "\n",
    "         # Adversarial ground truths\n",
    "        valid = Variable(torch.FloatTensor(np.ones((real_X_A.size(0), *patch))).cuda(), requires_grad=False)\n",
    "        fake = Variable(torch.FloatTensor(np.zeros((real_X_A.size(0), *patch))).cuda(), requires_grad=False)\n",
    "\n",
    "        #  Train G_A and G_B\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(G__BA(real_X_A), real_X_A)\n",
    "        loss_id_B = criterion_identity(G__AB(real_Y_B), real_Y_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_X_B = G__AB(real_X_A)\n",
    "        pred_fake = D__B(fake_X_B)\n",
    "        #print(pred_fake.shape,valid.shape)\n",
    "        loss_GAN_AB = criterion_GAN(pred_fake, valid)\n",
    "\n",
    "        fake_Y_A = G__BA(real_Y_B)\n",
    "        pred_fake = D__A(fake_Y_A)\n",
    "        loss_GAN_BA = criterion_GAN(pred_fake, valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_X_A = G__BA(fake_X_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_X_A, real_X_A)\n",
    "        recov_Y_B = G__AB(fake_Y_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_Y_B, real_Y_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G =    loss_GAN + \\\n",
    "                    args.lambda_cyc * loss_cycle + \\\n",
    "                    args.lambda_id * loss_identity\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        #  Train D_A\n",
    "        \n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D__A(real_X_A)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_Y_A_ = fake_Y_A_buffer.push_and_pop(fake_Y_A)\n",
    "        pred_fake = D__A(fake_Y_A_.detach())\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        #  Train D_B\n",
    "        \n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = D__B(real_Y_B)\n",
    "        loss_real = criterion_GAN(pred_real, valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_X_B_ = fake_X_B_buffer.push_and_pop(fake_X_B)\n",
    "        pred_fake = D__B(fake_X_B_.detach())\n",
    "        loss_fake = criterion_GAN(pred_fake, fake)\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(train_dataloader) + i\n",
    "        batches_left = args.epoch_num * len(train_dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\" %\n",
    "                                                        (epoch+1, args.epoch_num,\n",
    "                                                        i, len(train_dataloader),\n",
    "                                                        loss_D.data.cpu(), loss_G.data.cpu(),\n",
    "                                                        loss_GAN.data.cpu(), loss_cycle.data.cpu(),\n",
    "                                                        loss_identity.data.cpu(), time_left))\n",
    "        \n",
    "        # Save training statistics\n",
    "        epoch_statistic += 'Batch:%d D_loss:%f G_loss:%f adv:%f cycle:%f identity:%f\\n' % (i,\n",
    "                                                        loss_D.data.cpu(), loss_G.data.cpu(),\n",
    "                                                        loss_GAN.data.cpu(), loss_cycle.data.cpu(),\n",
    "                                                        loss_identity.data.cpu())\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % args.sample_interval == 0:\n",
    "            sample_images(args,G__AB,G__BA, test_dataloader, epoch, batches_done)\n",
    "\n",
    "    stat_path = './%s-%s/statistics/' % (args.exp_name, args.dataset_name)\n",
    "    if not os.path.isdir(stat_path):\n",
    "        os.mkdir(stat_path)\n",
    "    f = open(stat_path + str(epoch), 'w')\n",
    "    f.write(epoch_statistic)\n",
    "    f.close()\n",
    "        \n",
    "    \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step(epoch)\n",
    "    lr_scheduler_D_B.step(epoch)\n",
    "    lr_scheduler_D_A.step(epoch)\n",
    "\n",
    "    if args.checkpoint_interval != -1 and epoch % args.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(G__AB.state_dict(), './%s-%s/%s/G__AB_%d.pth' % (args.exp_name, args.dataset_name, args.model_result_dir, epoch))\n",
    "        torch.save(G__BA.state_dict(), './%s-%s/%s/G__BA_%d.pth' % (args.exp_name, args.dataset_name, args.model_result_dir, epoch))\n",
    "        torch.save(D__A.state_dict(), './%s-%s/%s/D__A_%d.pth' % (args.exp_name, args.dataset_name, args.model_result_dir, epoch))\n",
    "        torch.save(D__B.state_dict(), './%s-%s/%s/D__B_%d.pth' % (args.exp_name, args.dataset_name, args.model_result_dir, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
